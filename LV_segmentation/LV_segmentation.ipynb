{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LV_segmentation.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"eZe8vekln9j6","colab_type":"text"},"source":["# Neural network for segmenting LV of heart\n","\n","## Introduction\n","This work was performed as part of the MRAI workshop (MIDL 2019 satellite meeting) with exercises designed by Esther Puyol (https://github.com/estherpuyol/MRAI_workshop).\n","\n","### Objective\n","Train a simple deep neural network (DNN) to classify between healthy and heart failure subjects using clinical metrics, i.e. LVEDV, LVESV, LVSV and LVEF."]},{"cell_type":"markdown","metadata":{"id":"MQirRpaep-kM","colab_type":"text"},"source":["## Import modules\n","The network is built using functions and classes from the [pytorch library](https://pytorch.org/docs/stable/index.html) "]},{"cell_type":"code","metadata":{"id":"og-H1NZBqaFE","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import pylab as plt#\n","import torch, torch.nn as nn, torch.nn.functional as F\n","from sklearn.metrics import balanced_accuracy_score\n","from sklearn.metrics import precision_score, recall_score\n","from sklearn.model_selection import StratifiedShuffleSplit"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P-2sznKCpSGC","colab_type":"text"},"source":["## Download dataset\n","The data for this study is from the [Sunnybrook Cardiac Data](https://www.cardiacatlas.org/studies/sunnybrook-cardiac-data/)\n","\n","A preprocessed subset of this data is used, where the data is filtered to contain only left ventricle myocardium segmentations and reduced in XY dimensions."]},{"cell_type":"code","metadata":{"id":"-B0W1zJKpPdk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"bd5706bb-6c02-4fcc-a396-8eccec3ab288","executionInfo":{"status":"ok","timestamp":1564588935877,"user_tz":-60,"elapsed":808,"user":{"displayName":"Laurence Jackson","photoUrl":"https://lh4.googleusercontent.com/-7DuU4tbtAgI/AAAAAAAAAAI/AAAAAAAABA0/VkmK1bbpYbQ/s64/photo.jpg","userId":"10906345623450179926"}}},"source":["![ -f scd_lvsegs.npz ] || wget https://github.com/estherpuyol/MRAI_workshop/raw/master/scd_lvsegs.npz\n","\n","data=np.load('scd_lvsegs.npz') # load all the data from the archive\n","\n","images=data['images'] # images in BHW array order : .shape (420, 64, 64)]\n","segs.shape=data['segs'] # segmentations in BHW array order : .shape (420, 64, 64)\n","caseIndices=data['caseIndices'] # the indices in `images` for each case : .shape (45, 2)\n","\n","images=images.astype(np.float32)/images.max() # normalize images"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(45, 2)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"A2mh117Tqibl","colab_type":"text"},"source":["## Split training and test data\n","This code block splits the data set into training and test data where the variable n_training determines the number of test cases.  "]},{"cell_type":"code","metadata":{"id":"0jQ8Ti5ZspKA","colab_type":"code","colab":{}},"source":["n_training = 6\n","\n","testIndex=caseIndices[-n_training,0] # keep the last n_training cases for testing\n","\n","# divide the images, segmentations, and categories into train/test sets\n","trainImages,trainSegs=images[:testIndex],segs[:testIndex]\n","testImages,testSegs=images[testIndex:],segs[testIndex:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q1uDuNzItFQx","colab_type":"text"},"source":["## Define segmentation network\n","Here two classes are defined. \n","\n","The first is the loss function ```DiceLoss``` which will provide a measure of overlap between ground truth segmentations and the network outputs. \n","\n","The second class ```SegNet``` is our artifical neural network, in this case an auto-encoder that inherits methods from the base NN class torch.nn.Module. "]},{"cell_type":"code","metadata":{"id":"yoqHiPFytdg9","colab_type":"code","colab":{}},"source":["class DiceLoss(nn.modules.loss._Loss):\n","    '''This defines the binary dice loss function used to assess segmentation overlap.'''\n","    def forward(self, source, target, smooth=1e-5):\n","        batchsize = target.size(0)\n","        source=source.sigmoid() # apply sigmoid to the source logits to impose it onto the [0,1] interval\n","        \n","        # flatten target and source arrays to 2D BV arrays\n","        tsum=target.view(batchsize, -1) \n","        psum=source.view(batchsize, -1)\n","        \n","        intersection=psum*tsum\n","        sums=psum+tsum \n","\n","        # compute the score, the `smooth` value is used to smooth results and prevent divide-by-zero\n","        score = 2.0 * (intersection.sum(1) + smooth) / (sums.sum(1) + smooth)\n","        \n","        # `score` is 1 for perfectly identical source and target, 0 for entirely disjoint\n","        return 1 - score.sum() / batchsize\n","\n","# Segmentation network\n","class SegNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.model=nn.Sequential(\n","            # layer 1: convolution, normalization, downsampling\n","            nn.Conv2d(1,2,3,1,1),\n","            nn.BatchNorm2d(2),\n","            nn.ReLU(),\n","            nn.MaxPool2d(3,2,1),\n","            # layer 2\n","            nn.Conv2d(2,4,3,1,1),\n","            # layer 3\n","            nn.ConvTranspose2d(4,2,3,2,1,1),\n","            nn.BatchNorm2d(2),\n","            nn.ReLU(),\n","            # layer 4: output\n","            nn.Conv2d(2,1,3,1,1),\n","        )\n","        \n","    def forward(self,x):\n","        return self.model(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YOfRqQ4Kt7MF","colab_type":"text"},"source":["## Train network"]},{"cell_type":"code","metadata":{"id":"Rw0dMKb-t_FJ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}